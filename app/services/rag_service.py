import time
import asyncio
from typing import Dict, Any, AsyncGenerator

from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

from app.core.config import get_settings
from app.core.exceptions import LLMConnectionError, MissingEnvironmentVariableError
from app.core.logging import get_logger

settings = get_settings()
logger = get_logger()

class FirstTokenTimeHandler:
    """Handler for tracking LLM generation metrics like time to first token and generation speed"""
    def __init__(self):
        self.first_token_time = None
        self.generation_start_time = None
        self.full_response = ""
        self.token_count = 0
        
    def start_generation(self):
        """Start timing the generation process"""
        self.generation_start_time = time.time()
        
    def handle_token(self, token: str):
        """
        Process each token as it's generated
        
        Args:
            token (str): The token generated by the LLM
        """
        # Record time of first token
        if self.first_token_time is None:
            self.first_token_time = time.time()
        
        # Update token count (approximation)
        self.token_count += 1
        
        # Build full response
        self.full_response += token
        
    def get_time_metrics(self) -> Dict[str, float]:
        """
        Get timing metrics for the generation process
        
        Returns:
            Dict containing timing metrics
        """
        if self.first_token_time is None:
            return {
                "time_to_first_token": 0, 
                "total_generation_time": 0,
                "tokens": 0,
                "tokens_per_second": 0
            }
            
        time_to_first_token = self.first_token_time - self.generation_start_time
        total_generation_time = time.time() - self.generation_start_time
        
        # Calculate tokens per second (excluding time to first token)
        tokens = self.token_count
        if tokens > 0 and total_generation_time > time_to_first_token:
            # Only count tokens after the first token appears
            generation_time_after_first = total_generation_time - time_to_first_token
            if generation_time_after_first > 0:
                tokens_per_second = (tokens - 1) / generation_time_after_first
            else:
                tokens_per_second = 0
        else:
            tokens_per_second = 0
            
        return {
            "time_to_first_token": time_to_first_token,
            "total_generation_time": total_generation_time,
            "tokens": tokens,
            "tokens_per_second": tokens_per_second
        }

class RagService:
    """Service for RAG operations including LLM interactions"""
    
    # Define standard template for document analysis
    TEMPLATE = """
    You are an expert researcher helping to answer questions based on the provided documents.

    Here is relevant information from the documents: {vector_search}

    Question: {question}

    Please answer the question comprehensively based on the information from the documents. If the information is not available in the documents, say so clearly.
    """
    
    def __init__(self):
        """Initialize the RAG service with LLM connection"""
        self.model = None
        self.prompt = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """Initialize connection to the LLM service"""
        # Check for required environment variables
        required_vars = ["OLLAMA_LLM_MODEL", "OLLAMA_LLM_BASE_URL"]
        missing_vars = [var for var in required_vars if not getattr(settings, var, None)]
        if missing_vars:
            raise MissingEnvironmentVariableError(missing_vars)
        
        # Initialize the LLM with streaming enabled
        try:
            self.model = OllamaLLM(
                model=settings.OLLAMA_LLM_MODEL,
                base_url=settings.OLLAMA_LLM_BASE_URL,
                streaming=True  # Enable streaming for token-by-token generation
            )
        except Exception as e:
            raise LLMConnectionError(f"Error initializing Ollama LLM: {str(e)}")
            
        # Create the prompt template
        self.prompt = ChatPromptTemplate.from_template(self.TEMPLATE)
    
    def generate_answer(self, question: str, vector_search_results: str) -> Dict[str, Any]:
        """
        Generate an answer to the question based on vector search results
        
        Args:
            question (str): User's question
            vector_search_results (str): Retrieved document contexts
            
        Returns:
            Dict containing the answer and timing metrics
        """
        # Create a token handler for this response
        token_handler = FirstTokenTimeHandler()
        
        # Start timing the generation
        token_handler.start_generation()
        
        # Format the prompt with the question and vector search results
        formatted_prompt = self.prompt.format(vector_search=vector_search_results, question=question)
        
        try:
            # Stream the response through the handler
            for chunk in self.model.stream(formatted_prompt):
                token_handler.handle_token(chunk)
                
            # Get the complete answer and metrics
            answer = token_handler.full_response
            metrics = token_handler.get_time_metrics()
            
            return {
                "answer": answer,
                "metrics": metrics
            }
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}", exc_info=True)
            raise LLMConnectionError(f"Error generating response: {str(e)}")
            
    async def generate_answer_stream(self, question: str, vector_search_results: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Generate a streaming answer to the question based on vector search results
        
        Args:
            question (str): User's question
            vector_search_results (str): Retrieved document contexts
            
        Yields:
            Dict containing token and timing information
        """
        # Create a token handler for this response
        token_handler = FirstTokenTimeHandler()
        
        # Start timing the generation
        token_handler.start_generation()
        
        # Format the prompt with the question and vector search results
        formatted_prompt = self.prompt.format(vector_search=vector_search_results, question=question)
        
        try:
            is_first_token = True
            token_count = 0
            
            # Stream the response through the handler and yield tokens
            for chunk in self.model.stream(formatted_prompt):
                token_handler.handle_token(chunk)
                token_count += 1
                
                # For the first token, include time_to_first_token metric
                if is_first_token:
                    is_first_token = False
                    time_to_first = time.time() - token_handler.generation_start_time
                    
                    yield {
                        "event": "token",
                        "data": {
                            "token": chunk,
                            "is_first": True,
                            "time_to_first_token": time_to_first
                        }
                    }
                else:
                    # For regular tokens, just yield the token
                    yield {
                        "event": "token",
                        "data": {
                            "token": chunk,
                            "is_first": False
                        }
                    }
                
                # Allow other tasks to run
                await asyncio.sleep(0)
            
            # Yield final metrics at the end
            metrics = token_handler.get_time_metrics()
            yield {
                "event": "completion",
                "data": {
                    "metrics": {
                        "total_generation_time": metrics["total_generation_time"],
                        "tokens": metrics["tokens"],
                        "tokens_per_second": metrics["tokens_per_second"]
                    },
                    "full_text": token_handler.full_response
                }
            }
            
        except Exception as e:
            logger.error(f"Error in streaming response generation: {str(e)}", exc_info=True)
            yield {
                "event": "error",
                "data": {
                    "message": f"Error generating streaming response: {str(e)}"
                }
            }